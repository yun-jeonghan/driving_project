import sys
import os
import torch
import numpy as np
import cv2
from pathlib import Path
from typing import List, Dict, Any

# [SRS 4.1.2] ÏãúÍ≥ÑÏó¥ Î∂ÑÏÑùÏùÑ ÏúÑÌïú ÌîÑÎ†àÏûÑ Î≤ÑÌçº ÌÅ¥ÎûòÏä§ (FIFO Queue)
class VideoBuffer:
    def __init__(self, size: int = 5):
        self.size = size
        self.buffer = []

    def push(self, frame: np.ndarray):
        """ÏÉà ÌîÑÎ†àÏûÑÏùÑ ÎÑ£Í≥†, ÏÇ¨Ïù¥Ï¶àÎ•º Ï¥àÍ≥ºÌïòÎ©¥ Í∞ÄÏû• Ïò§ÎûòÎêú ÌîÑÎ†àÏûÑ ÏÇ≠Ï†ú"""
        self.buffer.append(frame)
        if len(self.buffer) > self.size:
            self.buffer.pop(0)

    def is_ready(self) -> bool:
        """[REQ-FUNC-201] Î≤ÑÌçºÍ∞Ä Í∞ÄÎìù Ï∞ºÎäîÏßÄ ÌôïÏù∏"""
        return len(self.buffer) == self.size

    def get_all(self) -> List[np.ndarray]:
        return self.buffer

# [SRS 4.1.3] YOLOv8 Í∞ùÏ≤¥ ÌÉêÏßÄ Î∞è Í±∞Î¶¨ Ï∂îÏ†ï Ìï∏Îì§Îü¨
class YOLOHandler:
    def __init__(self, path: Path):
        from ultralytics import YOLO
        # Î™®Îç∏ ÌååÏùº Ï°¥Ïû¨ ÌôïÏù∏
        if not path.exists():
            print(f"‚ö†Ô∏è YOLO Î™®Îç∏ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {path}")
        self.model = YOLO(str(path))

    def track(self, frame: np.ndarray):
        """[REQ-FUNC-101] Ïã§ÏãúÍ∞Ñ Í∞ùÏ≤¥ Ï∂îÏ†Å ÏàòÌñâ (ID Ïú†ÏßÄ)"""
        return self.model.track(frame, persist=True, verbose=False)[0]

# [SRS 4.2.3] FastVGGT ÍπäÏù¥ Î∞è Îß•ÎùΩ Î∂ÑÏÑù Ìï∏Îì§Îü¨
class FastVGGTHandler:
    def __init__(self, path: Path, device: str):
        self.device = device
        # [SRS 2.4] FastVGGT Î†àÌè¨ÏßÄÌÜ†Î¶¨ Í≤ΩÎ°ú Ï∂îÍ∞Ä
        REPO_PATH = Path(__file__).resolve().parents[2] / "FastVGGT_repo"
        if str(REPO_PATH) not in sys.path:
            sys.path.insert(0, str(REPO_PATH))

        # üî• [CRITICAL] ÏÜåÏä§ÏΩîÎìú Î†àÎ≤®ÏóêÏÑú chunk_size 0 ÏóêÎü¨ ÏõêÏ≤ú Î¥âÏáÑ
        # TorchScript ÌôòÍ≤ΩÏóêÏÑúÎèÑ ÏûëÎèôÌïòÎèÑÎ°ù ÏóêÎü¨ Î∞úÏÉù ÏßÄÏ†êÏùò ÏÜåÏä§ÏΩîÎìúÎ•º ÏßÅÏ†ë ÏàòÏ†ïÌï©ÎãàÎã§.
        merge_file = REPO_PATH / "merging" / "merge.py"
        if merge_file.exists():
            try:
                with open(merge_file, "r", encoding='utf-8') as f:
                    content = f.read()
                
                # ÏóêÎü¨ ÏßÄÏ†ê: range(0, num_src, chunk_size) -> 0Ïùº Îïå 1024Î°ú ÏûëÎèôÌïòÍ≤å Î≥ÄÍ≤Ω
                target_line = "range(0, num_src, chunk_size)"
                fixed_line = "range(0, num_src, (chunk_size if chunk_size > 0 else 1024))"
                
                if target_line in content:
                    new_content = content.replace(target_line, fixed_line)
                    with open(merge_file, "w", encoding='utf-8') as f:
                        f.write(new_content)
                    print("üõ† [Fix] merge.py ÏÜåÏä§ÏΩîÎìú ÏßÅÏ†ë ÏàòÏà† ÏôÑÎ£å (chunk_size 0 Î∞©Ïñ¥)")
            except Exception as e:
                print(f"‚ö†Ô∏è ÏÜåÏä§ÏΩîÎìú ÏàòÏà† Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")

        try:
            # ÌååÏùº ÏàòÏ†ï ÌõÑ Î™®Îç∏ ÏûÑÌè¨Ìä∏
            from vggt.models.vggt import VGGT
            self.model = VGGT()
            
            if not path.exists():
                raise FileNotFoundError(f"Í∞ÄÏ§ëÏπò ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§: {path}")

            checkpoint = torch.load(str(path), map_location=self.device)
            state_dict = checkpoint.get('model', checkpoint.get('state_dict', checkpoint))
            self.model.load_state_dict(state_dict, strict=False)
            self.model.to(self.device).eval()

            # Ïù∏Ïä§ÌÑ¥Ïä§ ÏÜçÏÑ±ÎèÑ ÏïàÏ†ÑÌïòÍ≤å 1024Î°ú Ï£ºÏûÖ
            self._patch_all_chunk_sizes(1024)
            
            print("‚úÖ FastVGGT Î™®Îç∏ Î°úÎìú ÏôÑÎ£å")
        except Exception as e:
            print(f"‚ùå FastVGGT Î°úÎìú Ïã§Ìå®: {e}")
            self.model = None

    def _patch_all_chunk_sizes(self, value: int):
        """Î™®Îç∏ ÎÇ¥Î∂Ä Î™®Îì† Î™®ÎìàÏùò chunk_size ÏÜçÏÑ±ÏùÑ Ïû¨Í∑ÄÏ†ÅÏúºÎ°ú ÏàòÏ†ï"""
        if self.model is None: return
        self.model.chunk_size = value
        for m in self.model.modules():
            if hasattr(m, 'chunk_size'):
                m.chunk_size = value

    @torch.no_grad()
    def get_depth_map(self, frame: np.ndarray) -> np.ndarray:
        """[SRS 3.3] Ï†ÑÏ≤òÎ¶¨ ÌõÑ ÍπäÏù¥ Îßµ(Depth Map) ÏÉùÏÑ±"""
        if self.model is None:
            return np.ones((frame.shape[0], frame.shape[1]), dtype=np.float32) * 50.0

        # Ï†ÑÏ≤òÎ¶¨: 518x392 Ìï¥ÏÉÅÎèÑÎ°ú Î¶¨ÏÇ¨Ïù¥Ïßï
        img_input = cv2.resize(frame, (518, 392))
        img_tensor = torch.from_numpy(img_input).permute(2, 0, 1).float().to(self.device) / 255.0
        img_tensor = img_tensor.unsqueeze(0)
        
        # bf16 Î∞è ÌòºÌï© Ï†ïÎ∞ÄÎèÑ ÏÇ¨Ïö© (T4 GPU Í∞ÄÏÜç)
        with torch.cuda.amp.autocast(enabled=True, dtype=torch.bfloat16):
            output = self.model(img_tensor)
        
        # Ï∂úÎ†• ÌÖêÏÑú ÌååÏã±
        if isinstance(output, dict):
            depth = output.get('depth', list(output.values())[0])
        elif isinstance(output, (list, tuple)):
            depth = output[0]
        else:
            depth = output
            
        if torch.is_tensor(depth):
            return depth.squeeze().float().cpu().numpy()
        return depth

# [SRS 4.1] ÌÜµÌï© Ï£ºÌñâ Î∂ÑÏÑùÍ∏∞ (Orchestrator)
class DrivingAnalyzer:
    def __init__(self, yolo_path: Path, vggt_path: Path):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.yolo = YOLOHandler(yolo_path)
        self.vggt = FastVGGTHandler(vggt_path, self.device)
        self.buffer = VideoBuffer(size=5)
        self.history = {}

    def analyze_frame(self, frame: np.ndarray, timestamp: float) -> List[Dict[str, Any]]:
        """[SRS 4.1] Ïã§ÏãúÍ∞Ñ ÌîÑÎ†àÏûÑ Î∂ÑÏÑù Î∞è ÏúÑÌóò Ï†êÏàò ÏÇ∞Ï∂ú"""
        self.buffer.push(frame)
        
        # 1. Í∞ùÏ≤¥ ÌÉêÏßÄ
        yolo_res = self.yolo.track(frame)
        
        # 2. ÍπäÏù¥ Îßµ ÏÉùÏÑ±
        depth_map = self.vggt.get_depth_map(frame)
        if depth_map is None or not isinstance(depth_map, np.ndarray):
            depth_map = np.ones((frame.shape[0], frame.shape[1]), dtype=np.float32) * 50.0
            
        depth_map_resized = cv2.resize(depth_map, (frame.shape[1], frame.shape[0]))
        
        frame_report = []
        if hasattr(yolo_res, 'boxes') and yolo_res.boxes is not None:
            boxes = yolo_res.boxes.xyxy.cpu().numpy()
            clss = yolo_res.boxes.cls.cpu().numpy().astype(int)
            ids = yolo_res.boxes.id.cpu().numpy().astype(int) if yolo_res.boxes.id is not None else [-1] * len(boxes)

            for box, obj_id, cls in zip(boxes, ids, clss):
                x1, y1, x2, y2 = map(int, box)
                y1, y2 = max(0, y1), min(frame.shape[0], y2)
                x1, x2 = max(0, x1), min(frame.shape[1], x2)
                
                # ROI ÌèâÍ∑† Í±∞Î¶¨ Í≥ÑÏÇ∞
                roi_depth = depth_map_resized[y1:y2, x1:x2]
                curr_dist = np.mean(roi_depth) if roi_depth.size > 0 else 50.0
                
                # ÏÉÅÎåÄ ÏÜçÎèÑ Í≥ÑÏÇ∞
                velocity = 0.0
                if obj_id != -1 and obj_id in self.history:
                    prev = self.history[obj_id]
                    delta_t = timestamp - prev['time']
                    if delta_t > 0:
                        velocity = (prev['dist'] - curr_dist) / delta_t
                
                # ÏúÑÌóò Ï†êÏàò ÏÇ∞Ï∂ú (SRS 4.2.2 Í≥µÏãù)
                risk_score = min(100.0, (10.0 / (curr_dist + 1e-6)) * 10 + (max(0, velocity) * 15))
                alert = "DANGER" if risk_score >= 80.0 else "WARNING" if risk_score >= 50.0 else "NORMAL"

                if obj_id != -1:
                    self.history[obj_id] = {'dist': curr_dist, 'time': timestamp}
                
                frame_report.append({
                    "id": int(obj_id),
                    "label": self.yolo.model.names[cls],
                    "dist_m": round(float(curr_dist), 2),
                    "velocity_mps": round(float(velocity), 2),
                    "risk": round(risk_score, 1),
                    "alert": alert,
                    "bbox": [x1, y1, x2, y2]
                })
        return frame_report

    def draw_results(self, frame: np.ndarray, results: List[Dict]) -> np.ndarray:
        """[SRS 3.1] ÏãúÍ∞ÅÏ†Å Í≤ΩÍ≥† Î†àÏù¥Ïñ¥ Ïò§Î≤ÑÎ†àÏù¥"""
        annotated = frame.copy()
        for res in results:
            x1, y1, x2, y2 = res['bbox']
            color = (0, 0, 255) if res['alert'] == "DANGER" else (0, 165, 255) if res['alert'] == "WARNING" else (0, 255, 0)
            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
            cv2.putText(annotated, f"ID:{res['id']} {res['dist_m']}m", (x1, y1 - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            cv2.putText(annotated, f"Risk: {res['risk']}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        return annotated